{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a48832-50e9-4959-9868-204d4606927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d72a34-7fb4-4725-8434-e8fe3421672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.ImageFolder('imageset', data_transforms['train'])\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(test_dataset)}\n",
    "class_names = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8809a2-0c78-4492-aac8-5fb27438370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6f913d-356c-41f0-a47e-bd443c853cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.6161 Acc: 0.7391\n",
      "val Loss: 0.7830 Acc: 0.4167\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.6298 Acc: 0.7174\n",
      "val Loss: 0.9692 Acc: 0.5833\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.4672 Acc: 0.7609\n",
      "val Loss: 0.3768 Acc: 0.7500\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2892 Acc: 0.9130\n",
      "val Loss: 0.2087 Acc: 0.9167\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.2182 Acc: 0.9348\n",
      "val Loss: 0.5464 Acc: 0.9167\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.5380 Acc: 0.8261\n",
      "val Loss: 0.5835 Acc: 0.6667\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2201 Acc: 0.8696\n",
      "val Loss: 0.6265 Acc: 0.6667\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.5570 Acc: 0.8043\n",
      "val Loss: 0.2924 Acc: 0.8333\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.2745 Acc: 0.8261\n",
      "val Loss: 0.1746 Acc: 0.9167\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.2331 Acc: 0.8913\n",
      "val Loss: 0.4495 Acc: 0.8333\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.2042 Acc: 0.9130\n",
      "val Loss: 0.2283 Acc: 0.9167\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.3774 Acc: 0.8696\n",
      "val Loss: 0.4318 Acc: 0.8333\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.2273 Acc: 0.9130\n",
      "val Loss: 0.3090 Acc: 0.9167\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.2217 Acc: 0.8913\n",
      "val Loss: 0.1678 Acc: 0.9167\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.3807 Acc: 0.8261\n",
      "val Loss: 0.6074 Acc: 0.7500\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.2441 Acc: 0.8696\n",
      "val Loss: 0.1942 Acc: 0.9167\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.2805 Acc: 0.8913\n",
      "val Loss: 0.2828 Acc: 0.9167\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.2084 Acc: 0.9130\n",
      "val Loss: 0.2456 Acc: 0.9167\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1842 Acc: 0.9348\n",
      "val Loss: 0.3999 Acc: 0.8333\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.2981 Acc: 0.9130\n",
      "val Loss: 0.2818 Acc: 0.8333\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1946 Acc: 0.9783\n",
      "val Loss: 0.0980 Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.8913\n",
      "val Loss: 0.1494 Acc: 0.9167\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.3238 Acc: 0.8696\n",
      "val Loss: 0.2199 Acc: 0.9167\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.3831 Acc: 0.8261\n",
      "val Loss: 0.2474 Acc: 0.9167\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.3865 Acc: 0.8261\n",
      "val Loss: 0.0773 Acc: 1.0000\n",
      "\n",
      "Training complete in 17m 4s\n",
      "Best val Acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "import copy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()  \n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6dd1bd-a5b3-4be2-92d3-c69114bd0633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shrimp detected at 6.43 seconds.\n",
      "Shrimp detected at 11.48 seconds.\n",
      "Shrimp detected at 16.52 seconds.\n",
      "Shrimp detected at 21.57 seconds.\n",
      "Shrimp detected at 26.61 seconds.\n",
      "Shrimp detected at 31.65 seconds.\n",
      "Shrimp detected at 36.70 seconds.\n",
      "Shrimp detected at 41.74 seconds.\n",
      "Shrimp detected at 46.78 seconds.\n",
      "Shrimp detected at 51.83 seconds.\n",
      "Shrimp detected at 56.87 seconds.\n",
      "Shrimp detected at 61.91 seconds.\n",
      "Shrimp detected at 66.96 seconds.\n",
      "Shrimp detected at 72.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture('testvideo.mp4')\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS) \n",
    "\n",
    "detection_pause = 5\n",
    "frames_to_skip = detection_pause * frame_rate\n",
    "skip_frames_counter = 0 \n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if skip_frames_counter == 0:\n",
    "        frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        frame_transformed = data_transforms['val'](frame_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        outputs = model_ft(frame_transformed)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if preds[0] == 1: \n",
    "            current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000 \n",
    "            print(f\"Shrimp detected at {current_time:.2f} seconds.\")\n",
    "            skip_frames_counter = frames_to_skip \n",
    "\n",
    "    else:\n",
    "        skip_frames_counter -= 1 \n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d197d005-ee58-4028-bfd0-1ab414deec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dd39f8c-006a-45ba-85e2-5fd2bb0c6033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shrimp detected in the image.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "image_path = 'shrimp.png' \n",
    "image = Image.open(image_path).convert('RGB')  \n",
    "\n",
    "image_transformed = data_transforms['val'](image).unsqueeze(0).to(device)\n",
    "\n",
    "model_ft.eval() \n",
    "outputs = model_ft(image_transformed)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "if preds[0] == 1:  \n",
    "    print(\"Shrimp detected in the image.\")\n",
    "else:\n",
    "    print(\"No shrimp detected in the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c6c6b-3d3f-4ec5-91c2-03493e9c9509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
